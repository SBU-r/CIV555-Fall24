{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t9vk9Elugvmi"
   },
   "source": [
    "# **Convolution for MNIST-1D**\n",
    "\n",
    "This notebook investigates a 1D convolutional network for MNIST-1D as in figure 10.7 and 10.8a.\n",
    "\n",
    "Work through the cells below, running each cell in turn. In various places you will see the words \"TO DO\". Follow the instructions at these places and make predictions about what is going to happen or write code to complete the functions.\n",
    "\n",
    "Contact me at udlbookmail@gmail.com if you find any mistakes or have any suggestions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "D5yLObtZCi9J"
   },
   "outputs": [],
   "source": [
    "# Uncomment and run the following line to install MNIST 1D repository\n",
    "#!pip install git+https://github.com/greydanus/mnist1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment and run the following line for once if you use jupyter notebook and has never installed pytorch\n",
    "#!pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "YrXWAH7sUWvU"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib.pyplot as plt\n",
    "import mnist1d\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "twI72ZCrCt5z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded data from ./mnist1d_data.pkl\n",
      "Examples in training set: 4000\n",
      "Examples in test set: 1000\n",
      "Length of each example: 40\n"
     ]
    }
   ],
   "source": [
    "args = mnist1d.data.get_dataset_args()\n",
    "data = mnist1d.data.get_dataset(args, path='./mnist1d_data.pkl', download=False, regenerate=False)\n",
    "\n",
    "# The training and test input and outputs are in\n",
    "# data['x'], data['y'], data['x_test'], and data['y_test']\n",
    "print(\"Examples in training set: {}\".format(len(data['y'])))\n",
    "print(\"Examples in test set: {}\".format(len(data['y_test'])))\n",
    "print(\"Length of each example: {}\".format(data['x'].shape[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "8bKADvLHbiV5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 4000 examples (columns), each of which has 40 dimensions (rows)\n",
      "Validation data: 1000 examples (columns), each of which has 40 dimensions (rows)\n"
     ]
    }
   ],
   "source": [
    "# Load in the data\n",
    "train_data_x = data['x'].transpose()\n",
    "train_data_y = data['y']\n",
    "val_data_x = data['x_test'].transpose()\n",
    "val_data_y = data['y_test']\n",
    "# Print out sizes\n",
    "print(\"Train data: %d examples (columns), each of which has %d dimensions (rows)\"%((train_data_x.shape[1],train_data_x.shape[0])))\n",
    "print(\"Validation data: %d examples (columns), each of which has %d dimensions (rows)\"%((val_data_x.shape[1],val_data_x.shape[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_sFvRDGrl4qe"
   },
   "source": [
    "Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "FslroPJJffrh"
   },
   "outputs": [],
   "source": [
    "# There are 40 input dimensions and 10 output dimensions for this data\n",
    "# The inputs correspond to the 40 offsets in the MNIST1D template.\n",
    "D_i = 40\n",
    "# The outputs correspond to the 10 digits\n",
    "D_o = 10\n",
    "\n",
    "\n",
    "# Create a model with the following layers\n",
    "# 1. Convolutional layer, (input=length 40 and 1 channel, kernel size 3, stride 2, padding=\"valid\", 15 output channels )\n",
    "# 2. ReLU\n",
    "# 3. Convolutional layer, (input=length 19 and 15 channels, kernel size 3, stride 2, padding=\"valid\", 15 output channels )\n",
    "# 4. ReLU\n",
    "# 5. Convolutional layer, (input=length 9 and 15 channels, kernel size 3, stride 2, padding=\"valid\", 15 output channels)\n",
    "# 6. ReLU\n",
    "# 7. Flatten (converts 4x15) to length 60\n",
    "# 8. Linear layer (input size = 60, output size = 10)\n",
    "# References:\n",
    "# https://pytorch.org/docs/1.13/generated/torch.nn.Conv1d.html?highlight=conv1d#torch.nn.Conv1d\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html\n",
    "# https://pytorch.org/docs/1.13/generated/torch.nn.Linear.html?highlight=linear#torch.nn.Linear\n",
    "\n",
    "# NOTE THAT THE CONVOLUTIONAL LAYERS NEED TO TAKE THE NUMBER OF INPUT CHANNELS AS A PARAMETER\n",
    "# AND NOT THE INPUT SIZE.\n",
    "\n",
    "model = nn.Sequential(\n",
    "nn.Conv1d(in_channels=1, out_channels=15, kernel_size=3, stride=2, padding=0),  # Layer 1\n",
    "nn.ReLU(),  \n",
    "nn.Conv1d(in_channels=15, out_channels=15, kernel_size=3, stride=2, padding=0),  # Layer 2\n",
    "nn.ReLU(),  \n",
    "nn.Conv1d(in_channels=15, out_channels=15, kernel_size=3, stride=2, padding=0),  # Layer 3\n",
    "nn.ReLU(),  \n",
    "nn.Flatten(),  # Layer 4\n",
    "nn.Linear(in_features=60, out_features=10) # a fully-connected layer\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "YgLaex1pfhqz"
   },
   "outputs": [],
   "source": [
    "# He initialization of weights\n",
    "def weights_init(layer_in):\n",
    "  if isinstance(layer_in, nn.Linear):\n",
    "    nn.init.kaiming_uniform_(layer_in.weight)\n",
    "    layer_in.bias.data.fill_(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NYw8I_3mmX5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     0, train loss 1.963589, train error 75.78,  val loss 1.969101, percent error 77.40\n",
      "Epoch     1, train loss 1.507855, train error 60.58,  val loss 1.516950, percent error 63.60\n",
      "Epoch     2, train loss 1.500342, train error 60.17,  val loss 1.513188, percent error 61.80\n",
      "Epoch     3, train loss 1.378678, train error 57.83,  val loss 1.405297, percent error 58.30\n",
      "Epoch     4, train loss 1.202550, train error 47.15,  val loss 1.208313, percent error 50.70\n",
      "Epoch     5, train loss 1.089538, train error 42.35,  val loss 1.099923, percent error 43.20\n",
      "Epoch     6, train loss 1.120150, train error 44.75,  val loss 1.162835, percent error 47.40\n",
      "Epoch     7, train loss 1.004039, train error 39.12,  val loss 1.030008, percent error 41.90\n",
      "Epoch     8, train loss 0.845426, train error 34.22,  val loss 0.903300, percent error 37.30\n",
      "Epoch     9, train loss 0.804589, train error 30.90,  val loss 0.846870, percent error 35.60\n",
      "Epoch    10, train loss 0.791301, train error 31.72,  val loss 0.870046, percent error 37.10\n",
      "Epoch    11, train loss 0.687716, train error 28.00,  val loss 0.752798, percent error 31.90\n"
     ]
    }
   ],
   "source": [
    "# choose cross entropy loss function (equation 5.24 in the loss notes)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "# construct SGD optimizer and initialize learning rate and momentum\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.05, momentum=0.9)\n",
    "# object that decreases learning rate by half every 20 epochs\n",
    "scheduler = StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "# create 100 dummy data points and store in data loader class\n",
    "x_train = torch.tensor(train_data_x.transpose().astype('float32'))\n",
    "y_train = torch.tensor(train_data_y.astype('long')).long()\n",
    "x_val= torch.tensor(val_data_x.transpose().astype('float32'))\n",
    "y_val = torch.tensor(val_data_y.astype('long')).long()\n",
    "\n",
    "# load the data into a class that creates the batches\n",
    "data_loader = DataLoader(TensorDataset(x_train,y_train), batch_size=100, shuffle=True, worker_init_fn=np.random.seed(1))\n",
    "\n",
    "# Initialize model weights\n",
    "model.apply(weights_init)\n",
    "\n",
    "# loop over the dataset n_epoch times\n",
    "n_epoch = 100\n",
    "# store the loss and the % correct at each epoch\n",
    "losses_train = np.zeros((n_epoch))\n",
    "errors_train = np.zeros((n_epoch))\n",
    "losses_val = np.zeros((n_epoch))\n",
    "errors_val = np.zeros((n_epoch))\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "  # loop over batches\n",
    "  for i, data in enumerate(data_loader):\n",
    "    # retrieve inputs and labels for this batch\n",
    "    x_batch, y_batch = data\n",
    "    # zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "    # forward pass -- calculate model output\n",
    "    pred = model(x_batch[:,None,:])\n",
    "    # compute the loss\n",
    "    loss = loss_function(pred, y_batch)\n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    # SGD update\n",
    "    optimizer.step()\n",
    "\n",
    "  # Run whole dataset to get statistics -- normally wouldn't do this\n",
    "  pred_train = model(x_train[:,None,:])\n",
    "  pred_val = model(x_val[:,None,:])\n",
    "  _, predicted_train_class = torch.max(pred_train.data, 1)\n",
    "  _, predicted_val_class = torch.max(pred_val.data, 1)\n",
    "  errors_train[epoch] = 100 - 100 * (predicted_train_class == y_train).float().sum() / len(y_train)\n",
    "  errors_val[epoch]= 100 - 100 * (predicted_val_class == y_val).float().sum() / len(y_val)\n",
    "  losses_train[epoch] = loss_function(pred_train, y_train).item()\n",
    "  losses_val[epoch]= loss_function(pred_val, y_val).item()\n",
    "  print(f'Epoch {epoch:5d}, train loss {losses_train[epoch]:.6f}, train error {errors_train[epoch]:3.2f},  val loss {losses_val[epoch]:.6f}, percent error {errors_val[epoch]:3.2f}')\n",
    "\n",
    "  # tell scheduler to consider updating learning rate\n",
    "  scheduler.step()\n",
    "\n",
    "# Plot the results\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(errors_train,'r-',label='train')\n",
    "ax.plot(errors_val,'b-',label='validation')\n",
    "ax.set_ylim(0,100); ax.set_xlim(0,n_epoch)\n",
    "ax.set_xlabel('Epoch'); ax.set_ylabel('Error')\n",
    "ax.set_title('Part I: Validation Result %3.2f'%(errors_val[-1]))\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
